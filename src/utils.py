import datetime as dt
import pathlib
import subprocess
import torch
import argparse
from config import (
    DEFAULT_SCENE_THRESHOLD,
    DEFAULT_MAX_TOKENS,
    DEFAULT_MODEL_ID
)
from scenedetect import ContentDetector, FrameTimecode, detect
from typing import Tuple

def _sec_to_hhmmss(sec: float) -> str:
    return str(dt.timedelta(seconds=int(sec))).rjust(8, "0")


def _grab_frame(video: str, timestamp: float, out_path: pathlib.Path) -> bool:
    cmd = [
        "ffmpeg", "-y", "-loglevel", "error",
        "-ss", f"{timestamp:.3f}", "-i", video,
        "-frames:v", "1", str(out_path),
    ]
    try:
        subprocess.run(cmd, check=True)
        return True
    except subprocess.CalledProcessError as err:
        print(f"âš ï¸  ffmpeg error {err.returncode} â€” ÐºÐ°Ð´Ñ€ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½")
        return False

def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Smart scene captions generator (2025 edition)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:\n  python3 main.py clip.mp4\n  python3 main.py clip.mp4 0.15 --model Salesforce/blip-image-captioning-base""",
    )
    parser.add_argument("video")
    parser.add_argument("scene_thr", nargs="?", type=float, default=DEFAULT_SCENE_THRESHOLD,
                        help="Ð¿Ð¾Ñ€Ð¾Ð³: 0â€‘1 ÐºÐ°Ðº Ð´Ð¾Ð»Ñ Ð¸Ð»Ð¸ 0â€‘100 ÐºÐ°Ðº %%")
    parser.add_argument("max_tokens", nargs="?", type=int, default=DEFAULT_MAX_TOKENS)
    parser.add_argument("--model", default=DEFAULT_MODEL_ID,
                        help="HFâ€‘id Ð¼Ð¾Ð´ÐµÐ»Ð¸ imageâ€‘toâ€‘text (pipeline)")
    parser.add_argument("--keep-all", action="store_true", help="Ð½Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ñ‹Ðµ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐ¸")
    parser.add_argument("--debug", action="store_true", help="Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ñ‹Ð¹ Ð»Ð¾Ð³ Ð¸ progressâ€‘bar")
    return parser.parse_args()

# device select

def _select_device() -> str | int:
    if torch.cuda.is_available():
        print("ðŸ–¥  CUDA GPU")
        return 0
    if torch.backends.mps.is_available():
        print("ðŸ–¥  AppleÂ Mâ€‘series (MPS)")
        return "mps"
    if hasattr(torch, "xpu") and torch.xpu.is_available():  # IntelÂ GPU
        print("ðŸ–¥  IntelÂ XPU")
        return "xpu"
    print("ðŸ–¥  CPU")
    return -1

def _detect_scenes(video_path: str, threshold_raw: float, debug: bool) -> list[Tuple[float, float]]:
    # ÐŸÐ¾Ñ€Ð¾Ð³ Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°ÐµÐ¼ Ð»Ð¸Ð±Ð¾ ÐºÐ°ÐºÂ Ð´Ð¾Ð»ÑŽ (<=1), Ð»Ð¸Ð±Ð¾ ÐºÐ°ÐºÂ Ñ†ÐµÐ»Ð¾Ðµ 0â€‘100
    thr_pct = int(threshold_raw * 100) if threshold_raw <= 1 else int(threshold_raw)
    detector = ContentDetector(threshold=thr_pct)
    scene_list = detect(video_path, detector, show_progress=debug)
    scenes_sec: list[Tuple[float, float]] = [
        (start.get_seconds(), end.get_seconds())  # type: ignore[attr-defined]
        for start, end in scene_list
    ]
    if debug:
        print(f"      Ð”ÐµÑ‚ÐµÐºÑ‚Ð¾Ñ€ Ð²ÐµÑ€Ð½ÑƒÐ» {len(scenes_sec)} ÑÑ†ÐµÐ½ Ð¿Ñ€Ð¸ threshold={thr_pct}")
    return scenes_sec

def _norm(txt: str) -> str:
        return " ".join(txt.casefold().split())
